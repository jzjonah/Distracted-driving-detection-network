Justification:\
The Model I implemented is a reduction of the 2012 AlexNet. AlexNet was the first convolutional neural network to win the ImageNet Large Scale Visual Recognition Challenge and started the vast research into convolutional neural networks for image classification. The other model considered was the YOLOv3 model. YOLOv3 is a model that is known for its speed in identifying multiple objects in an image and where they are located. This is something that the YOLOv3 algorithm has over ALexNet but it comes due to a much more complex architecture. This larger architecture would mean much longer training times and the larger size can also become prone to overtraining if the complexity is unneeded for the project. Since our project
 
relies on the idea that the input will be coming from a dash cam located in the same spot of each car the ability to determine where in the image the object is found is not needed as the driver that is being analyzed will alway be in the same location within the image. Also when looking at the differences within the training data the differences between each class is quite distinct. For example the non distracted drivers have their face towards the road and both hands on the wheel, while the examples of distracted driving have the driver either looking away from the road, drinking, or on the phone. These are all large differences between the data making it easier for the model to figure out compared to many of the most complex image classification contests that ask for small differences between many different potential classes, for example being able to identify each breed of dog. Since the project was seen as a relatively simple image classification problem where the point of focus is in relatively the same spot for the whole dataset, the added complexity of the YOLOv3 model was determined to not be beneficial and result only in longer training times and potential over fitting. Since this would cause less of an ability to fine tune the parameters of the training it was determined that the YOLOv3 would be a poor choice of model for the project. In contrast AlexNet’s original architecture is made up of 10 layers, allowing for relatively fast training time and a complexity more matching the complexity of the problem.



Model architecture:\
The original architecture of the AlexNet was revolutionary within the field of artificial intelligence as it proved that the use of multiple convolutional layers then followed by fully connected dense layers could outperform all the industry standards at that time. This proof of concept is what led to CNN’s dominating the image classification projects till this day and the original architecture still holds up. The AlexNet architecture includes convolutional layers, max-pooling layers, and dense layers. For my model I made certain changes that proved to output more accurate results. These changes include changes to the image preprocessing and simplifying of the network's architecture.

Image Preprocessing:\
The image preprocessing this model focused on reduction of image size and data normalization. To reduce the size of the images they were changed from colour to black and white and resized to be 160 by 120 from 640 by 480. This reduces the required amount of nodes within the model, specifically reducing the input layer by 7 times, and therefore reduces the training time. To normalize the data all values were divided by 255, since the largest colour value is 255 this will make all the data points to be between 0-1. This is the industry standard for image classification due to its proven better results. The data is also changed to be of data type float32 as this is needed for the data to run properly through the network.
 
Architecture:\
The original architecture of AlexNet can be seen in the figure below. First training was done with this architecture and from there, changes were made for the purpose of the assignment and also to attempt to improve the accuracy.
![image](https://github.com/jzjonah/Distracted-driving-detection-network/assets/55960435/aa449eaf-ff12-4419-94a6-f02df6480f9e)
The first change done was removing convolutional layers 3 through 5. This was done because the original AlexNet was designed to classify images from 1000 different options, some being much similar in look compared to the different classification option in this project. This would mean that much smaller features within the image would have to be extracted to determine what object it is, and therefore require more convolutional layers. In this project the extra Convolutional layers could do harm instead of good as they could pick up small things within the images in the training data that are independent of whether the driver is distracted or not and determine that to be apart of the object's identifier. This over training could result in misleading accuracy scores during training and a reduction in the testing scores. Next an extra dense layer was added to the end to make it 4 consecutive dense layers instead of three. This was done since it showed an improvement in the accuracy. The final architecture can be found in the figure below.\
![image](https://github.com/jzjonah/Distracted-driving-detection-network/assets/55960435/59b5976f-1c8e-4421-87a5-d201183e537f)
